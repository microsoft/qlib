order_file: ./data/orders/test_orders.pkl
start_time: "9:30"
end_time: "14:54"
qlib:
  provider_uri_5min: ./data/bin/
  feature_root_dir: ./data/pickle/
  feature_columns_today: [
    "$open", "$high", "$low", "$close", "$vwap", "$bid", "$ask", "$volume",
    "$bidV", "$bidV1", "$bidV3", "$bidV5", "$askV", "$askV1", "$askV3", "$askV5"
  ]
  feature_columns_yesterday: [
    "$open_1", "$high_1", "$low_1", "$close_1", "$vwap_1", "$bid_1", "$ask_1", "$volume_1",
    "$bidV_1", "$bidV1_1", "$bidV3_1", "$bidV5_1", "$askV_1", "$askV1_1", "$askV3_1", "$askV5_1"
  ]
exchange:
  limit_threshold: null
  deal_price: ["$close", "$close"]
  volume_threshold: null
strategies:
  1day:
    class: SAOEIntStrategy
    kwargs:
      data_granularity: 5
      action_interpreter:
        class: CategoricalActionInterpreter
        kwargs:
          max_step: 8
          values: 4
        module_path: qlib.rl.order_execution.interpreter
      network:
        class: Recurrent
        kwargs: {}
        module_path: qlib.rl.order_execution.network
      policy:
        class: PPO  # PPO, DQN
        kwargs:
          lr: 0.0001
          # Restore `weight_file` once the training workflow finishes. You can change the checkpoint file you want to use.
          # weight_file: outputs/opds/checkpoints/latest.pth
        module_path: qlib.rl.order_execution.policy
      state_interpreter:
        class: FullHistoryStateInterpreter
        kwargs:
          data_dim: 5
          data_ticks: 48
          max_step: 8
          processed_data_provider:
            class: PickleProcessedDataProvider
            kwargs:
              data_dir: ./data/pickle_dataframe/feature
            module_path: qlib.rl.data.pickle_styled
        module_path: qlib.rl.order_execution.interpreter
    module_path: qlib.rl.order_execution.strategy
  30min:
    class: TWAPStrategy
    kwargs: {}
    module_path: qlib.contrib.strategy.rule_strategy
concurrency: 16
output_dir: outputs/opds/
