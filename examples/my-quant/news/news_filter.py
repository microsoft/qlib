import os
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# ================= ğŸ›¡ï¸ è·¯å¾„ä¸ç¯å¢ƒé…ç½®åŒº =================
PROJECT_ROOT = r"D:\Quant-qlib-official"
MODEL_CACHE_DIR = os.path.join(PROJECT_ROOT, "data", "LLM-models")

# å¼ºåˆ¶åˆ›å»ºç›®å½•
if not os.path.exists(MODEL_CACHE_DIR):
    try:
        os.makedirs(MODEL_CACHE_DIR)
    except Exception:
        pass

os.environ["HF_HOME"] = MODEL_CACHE_DIR
os.environ["SENTENCE_TRANSFORMERS_HOME"] = MODEL_CACHE_DIR
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# =======================================================

from sentence_transformers import SentenceTransformer

class NewsRobuster:
    def __init__(self):
        print(f"æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹ (ç”¨äºå»é‡)...")
        try:
            self.model = SentenceTransformer(
                'paraphrase-multilingual-MiniLM-L12-v2', 
                cache_folder=MODEL_CACHE_DIR
            )
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
            
        # å­˜å‘é‡
        self.history_embeddings = []
        # [æ–°å¢] å­˜å¯¹åº”çš„æ ‡é¢˜ï¼Œç”¨æ¥å›æº¯æ˜¯è·Ÿè°é‡å¤äº†
        self.history_titles = []

    def check_duplicate(self, text, threshold=0.85):
        """
        è¿”å›: (æ˜¯å¦é‡å¤, ç›¸ä¼¼åº¦, åŸæ–‡æ ‡é¢˜)
        """
        if not self.history_embeddings:
            return False, 0.0, None
            
        current_emb = self.model.encode([text])
        
        # è®¡ç®—ä¸å†å²æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦
        similarities = cosine_similarity(current_emb, np.vstack(self.history_embeddings))
        
        # æ‰¾åˆ°ç›¸ä¼¼åº¦æœ€å¤§çš„é‚£ä¸ªç´¢å¼•
        max_sim_idx = np.argmax(similarities)
        max_sim = similarities[0][max_sim_idx]
        
        if max_sim > threshold:
            # æ‰¾åˆ°å¯¹åº”çš„å†å²æ ‡é¢˜
            source_title = self.history_titles[max_sim_idx]
            return True, max_sim, source_title
        
        return False, max_sim, None

    def process_batch(self, df):
        clean_news = []
        print(f"\nå¼€å§‹å¤„ç† {len(df)} æ¡æ–°é—» (æ— å…³é”®è¯è¿‡æ»¤æ¨¡å¼)...")
        
        # 1. æŒ‰æ—¶é—´æ­£åºæ’åº (ç¡®ä¿æœ€æ—©çš„æ¶ˆæ¯è¢«ä¿ç•™ï¼Œåé¢çš„è¢«è§†ä¸ºé‡å¤)
        # å°è¯•å¯»æ‰¾æ—¶é—´åˆ—
        time_col = None
        for col in ['å‘å¸ƒæ—¶é—´', 'æ—¶é—´', 'datetime', 'time']:
            if col in df.columns:
                time_col = col
                break
        
        if time_col:
            df = df.sort_values(by=time_col, ascending=True)
            print(f"å·²æŒ‰æ—¶é—´åˆ— '{time_col}' æ­£åºæ’åˆ—")
        
        for index, row in df.iterrows():
            title = str(row.get('æ–°é—»æ ‡é¢˜', row.get('æ ‡é¢˜', ''))) 
            content = str(row.get('æ–°é—»å†…å®¹', row.get('å†…å®¹', row.get('æ‘˜è¦', ''))))
            full_text = title + " " + content
            
            # --- æ ¸å¿ƒå»é‡é€»è¾‘ ---
            is_dup, sim_score, source_title = self.check_duplicate(full_text)
            
            if is_dup:
                print("-" * 60)
                print(f"â™»ï¸ [å‘ç°é‡å¤] ç›¸ä¼¼åº¦: {sim_score:.4f}")
                print(f"   å½“å‰æ–°é—»: {title[:30]}...")
                print(f"   é‡å¤æ¥æº: {source_title[:30]}...") # æ‰“å°å‡ºè·Ÿè°é‡å¤äº†
                print("-" * 60)
                continue
            
            # --- å…¥åº“ ---
            current_emb = self.model.encode([full_text])[0]
            self.history_embeddings.append(current_emb)
            self.history_titles.append(title) # åŒæ—¶å­˜å…¥æ ‡é¢˜
            
            # ä¿æŒçª—å£å¤§å°
            if len(self.history_embeddings) > 2000:
                self.history_embeddings.pop(0)
                self.history_titles.pop(0)
            
            clean_news.append(row)
            # print(f"âœ… [ä¿ç•™] {title[:20]}")

        return pd.DataFrame(clean_news)

if __name__ == "__main__":
    # âš ï¸ ä¿®æ”¹è¿™é‡Œçš„æ–‡ä»¶å
    input_csv = "data/macro_news/news_20260203_185837.csv" 
    
    # è·¯å¾„å¤„ç†
    base_dir = os.path.join(PROJECT_ROOT, "examples", "my-quant")
    full_input_path = os.path.join(base_dir, input_csv)
    if not os.path.exists(full_input_path):
        full_input_path = input_csv

    if os.path.exists(full_input_path):
        robuster = NewsRobuster()
        df = pd.read_csv(full_input_path)
        
        df_clean = robuster.process_batch(df)
        
        output_path = full_input_path.replace("news_", "clean_news_")
        df_clean.to_csv(output_path, index=False, encoding="utf-8-sig")
        
        print(f"\nğŸ‰ å®Œæˆï¼åŸå§‹: {len(df)} -> å‰©ä½™: {len(df_clean)}")
        print(f"æ–‡ä»¶: {output_path}")
    else:
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {full_input_path}")